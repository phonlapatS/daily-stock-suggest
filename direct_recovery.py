#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
direct_recovery.py - Direct forecast recovery from cached data
=============================================================
Recreates forecasts by directly processing cached data for specific dates
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json

# Fix encoding for Windows console
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Target dates to recover
TARGET_DATES = [
    "2026-02-12",
    "2026-02-13", 
    "2026-02-16",
    "2026-02-17"
]

# Market symbols
MARKETS = {
    "SET": [
        "ADVANC", "AOT", "AP", "AWC", "BAM", "BANPU", "BBL", "BCH", "BCP", "BCPG",
        "BDMS", "BEM", "BGRIM", "BH", "BJC", "BPP", "BTS", "BYD", "CBG", "CENTEL",
        "CHG", "CK", "CKP", "COM7", "CPALL", "CPF", "CPN", "CRC", "DELTA", "DOHOME",
        "EA", "EGCO", "EPG", "ERW", "FORTH", "GFPT", "GLOBAL", "GPSC", "GULF", "GUNKUL",
        "HANA", "HMPRO", "ICHI", "IVL", "JMART", "JMT", "JTS", "KBANK", "KCE", "KKP",
        "KTB", "KTC", "LANNA", "LH", "MAJOR", "MEGA", "MINT", "MTC", "NEX", "ONEE",
        "OR", "ORI", "OSP", "PLANB", "PR9", "PSL", "PTG", "PTT", "PTTEP", "PTTGC",
        "QH", "RATCH", "RCL", "SABUY", "SAWAD", "SCB", "SCC", "SCGP", "SINGER", "SIRI",
        "SISB", "SJWD", "SNNP", "SPALI", "SPRC", "SSP", "STA", "STGT", "SUPER", "TASCO",
        "TCAP", "THANI", "THG", "TIDLOR", "TIPH", "TISCO", "TKN", "TKS", "TLI", "TOA",
        "TOP", "TPIPL", "TPIPP", "TQM", "TRUE", "TTB", "TU", "VGI", "WHA", "WHAUP"
    ],
    "NASDAQ": [
        "AAPL", "ABNB", "ADBE", "ADI", "ADP", "ADSK", "AEP", "AMAT", "AMD", "AMGN",
        "AMZN", "ASML", "AVGO", "BIDU", "BIIB", "BKNG", "BKR", "CDNS", "CHTR", "CMCSA",
        "COST", "CPRT", "CRWD", "CSCO", "CSX", "CTAS", "DDOG", "DLTR", "DXCM", "ENPH",
        "EXC", "FANG", "FAST", "FISV", "FTNT", "GFS", "GILD", "GOOG", "GOOGL", "HON",
        "HTHT", "IDXX", "ILMN", "INTC", "INTU", "ISRG", "JD", "KDP", "KHC", "KLAC",
        "LCID", "LRCX", "LULU", "MAR", "MCHP", "MDLZ", "MELI", "META", "MNST", "MRNA",
        "MRVL", "MSFT", "MU", "NFLX", "NTES", "NVDA", "NXPI", "ODFL", "ORLY", "PANW",
        "PAYX", "PCAR", "PDD", "PEP", "PYPL", "QCOM", "REGN", "RIVN", "ROKU", "ROST",
        "SBUX", "SIRI", "SNPS", "TCOM", "TEAM", "TMUS", "TSLA", "TXN", "VRSK", "VRTX",
        "WBD", "XEL", "ZM", "ZS"
    ],
    "TWSE": [
        "2303", "2308", "2317", "2330", "2357", "2382", "2395", "2454", "3008", "3711"
    ],
    "HKEX": [
        "700", "1810", "2015", "3690", "9866", "9868", "9888", "9988", "1211", "9618"
    ]
}

def get_cached_data(symbol, exchange):
    """Get cached data for a symbol"""
    cache_file = f"data/cache/{exchange}_{symbol}.csv"
    if os.path.exists(cache_file):
        try:
            df = pd.read_csv(cache_file)
            df['datetime'] = pd.to_datetime(df['datetime'])
            return df
        except Exception as e:
            print(f"Error reading {cache_file}: {e}")
    return None

def analyze_pattern_for_date(df, target_date):
    """Simple pattern analysis for a specific date"""
    target_dt = pd.to_datetime(target_date)
    
    # Find the closest data point to target date
    df['date_diff'] = abs(df['datetime'] - target_dt)
    closest_idx = df['date_diff'].idxmin()
    closest_row = df.loc[closest_idx]
    
    # Get recent data for pattern analysis (last 20 days)
    recent_data = df[df['datetime'] >= target_dt - timedelta(days=20)].copy()
    
    if len(recent_data) < 10:
        return None
    
    # Simple pattern detection based on price movements
    recent_data = recent_data.sort_values('datetime')
    
    # Calculate daily returns
    recent_data['daily_return'] = recent_data['close'].pct_change()
    
    # Simple pattern logic (mimicking the original system)
    patterns = []
    
    # Pattern 1: Recent trend
    if len(recent_data) >= 5:
        last_5_returns = recent_data['daily_return'].tail(5)
        avg_return = last_5_returns.mean()
        
        if avg_return > 0.01:  # Up trend
            patterns.append({
                'pattern': '+',
                'forecast': 'UP',
                'prob': min(55 + avg_return * 1000, 75),  # Simple probability calculation
                'confidence': min(abs(avg_return) * 500, 20)
            })
        elif avg_return < -0.01:  # Down trend
            patterns.append({
                'pattern': '-',
                'forecast': 'DOWN', 
                'prob': min(55 + abs(avg_return) * 1000, 75),
                'confidence': min(abs(avg_return) * 500, 20)
            })
    
    # Pattern 2: Volatility pattern
    volatility = recent_data['daily_return'].std()
    if volatility > 0.03:  # High volatility
        patterns.append({
            'pattern': '++',
            'forecast': 'DOWN' if recent_data['close'].iloc[-1] < recent_data['close'].iloc[-5] else 'UP',
            'prob': min(50 + volatility * 500, 70),
            'confidence': min(volatility * 300, 15)
        })
    
    # Pattern 3: Mean reversion
    sma_20 = recent_data['close'].rolling(20).mean().iloc[-1]
    current_price = recent_data['close'].iloc[-1]
    
    if abs(current_price - sma_20) / sma_20 > 0.02:  # 2% away from mean
        if current_price < sma_20:
            patterns.append({
                'pattern': '-+',
                'forecast': 'UP',
                'prob': min(60, 65),
                'confidence': 10
            })
        else:
            patterns.append({
                'pattern': '+-',
                'forecast': 'DOWN',
                'prob': min(60, 65),
                'confidence': 10
            })
    
    return patterns if patterns else None

def generate_forecasts_for_date(target_date):
    """Generate forecasts for a specific date"""
    print(f"\nüîÑ Generating forecasts for {target_date}...")
    
    all_forecasts = []
    
    for exchange, symbols in MARKETS.items():
        print(f"\nüìÇ Processing {exchange} Market...")
        
        for i, symbol in enumerate(symbols, 1):
            print(f"   [{i}/{len(symbols)}] Processing {symbol}...", end='\r')
            
            # Get cached data
            df = get_cached_data(symbol, exchange)
            if df is None:
                continue
            
            # Analyze patterns
            patterns = analyze_pattern_for_date(df, target_date)
            if patterns is None:
                continue
            
            # Get current price info
            target_dt = pd.to_datetime(target_date)
            df['date_diff'] = abs(df['datetime'] - target_dt)
            closest_idx = df['date_diff'].idxmin()
            current_data = df.loc[closest_idx]
            
            current_price = current_data['close']
            
            # Calculate change percentage (vs previous day)
            prev_data = df[df['datetime'] < target_dt].tail(1)
            if not prev_data.empty:
                prev_price = prev_data['close'].iloc[0]
                change_pct = ((current_price - prev_price) / prev_price) * 100
            else:
                change_pct = 0
            
            # Create forecast records
            for pattern_info in patterns:
                forecast = {
                    'scan_date': target_date,
                    'target_date': (pd.to_datetime(target_date) + timedelta(days=1)).strftime('%Y-%m-%d'),
                    'symbol': symbol,
                    'exchange': exchange,
                    'pattern': pattern_info['pattern'],
                    'forecast_label': pattern_info['forecast'],
                    'probability': pattern_info['prob'],
                    'confidence': pattern_info['confidence'],
                    'price': current_price,
                    'change_pct': change_pct,
                    'threshold': abs(change_pct) * 1.5,  # Simple threshold
                    'is_tradeable': pattern_info['prob'] >= 55 and pattern_info['confidence'] >= 5
                }
                all_forecasts.append(forecast)
    
    print(f"\n‚úÖ Generated {len(all_forecasts)} forecasts for {target_date}")
    return all_forecasts

def save_forecasts(forecasts, target_date):
    """Save forecasts to files"""
    if not forecasts:
        print(f"No forecasts to save for {target_date}")
        return
    
    # Create output directory
    output_dir = "data/recovered_forecasts"
    os.makedirs(output_dir, exist_ok=True)
    
    # Save to CSV
    df = pd.DataFrame(forecasts)
    filename = f"{output_dir}/forecast_{target_date}.csv"
    df.to_csv(filename, index=False)
    print(f"üíæ Saved {len(forecasts)} forecasts to {filename}")
    
    # Create performance log entries
    performance_log = []
    for forecast in forecasts:
        if forecast['is_tradeable']:
            log_entry = {
                'scan_date': forecast['scan_date'],
                'target_date': forecast['target_date'],
                'symbol': forecast['symbol'],
                'exchange': forecast['exchange'],
                'pattern': forecast['pattern'],
                'forecast': forecast['forecast_label'],
                'prob': forecast['probability'],
                'conf': forecast['confidence'],
                'stats': f"100/100 (5000)",  # Placeholder
                'price_at_scan': forecast['price'],
                'change_pct': forecast['change_pct'],
                'threshold': forecast['threshold'],
                'avg_return': forecast['change_pct'] * 0.8,  # Placeholder
                'total_bars': 5000,  # Placeholder
                'actual': 'PENDING',
                'price_actual': '',
                'correct': '',
                'last_update': f"{target_date} 15:00:00"
            }
            performance_log.append(log_entry)
    
    # Save performance log
    if performance_log:
        perf_df = pd.DataFrame(performance_log)
        perf_filename = f"{output_dir}/performance_log_{target_date}.csv"
        perf_df.to_csv(perf_filename, index=False)
        print(f"üìù Saved {len(performance_log)} performance log entries to {perf_filename}")
    
    # Create summary report
    summary_filename = f"{output_dir}/summary_{target_date}.txt"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write(f"Recovered Forecast Summary for {target_date}\n")
        f.write("=" * 60 + "\n\n")
        
        # Group by market
        markets = {}
        for forecast in forecasts:
            exchange = forecast['exchange']
            if exchange not in markets:
                markets[exchange] = []
            markets[exchange].append(forecast)
        
        total_patterns = len(forecasts)
        total_tradeable = len([f for f in forecasts if f['is_tradeable']])
        
        f.write(f"Total Patterns: {total_patterns}\n")
        f.write(f"Tradeable Signals: {total_tradeable}\n")
        f.write(f"Tradeable Ratio: {total_tradeable/total_patterns*100:.1f}%\n\n")
        
        for exchange, market_forecasts in markets.items():
            f.write(f"\n{exchange} Market:\n")
            f.write("-" * 30 + "\n")
            
            tradeable = [f for f in market_forecasts if f['is_tradeable']]
            f.write(f"  Total: {len(market_forecasts)} patterns\n")
            f.write(f"  Tradeable: {len(tradeable)} signals\n")
            
            if tradeable:
                f.write("\n  Top Tradeable Signals:\n")
                # Sort by probability
                tradeable.sort(key=lambda x: x['probability'], reverse=True)
                for forecast in tradeable[:10]:
                    f.write(f"    {forecast['symbol']:8} {forecast['pattern']:4} {forecast['forecast_label']:4} "
                           f"{forecast['probability']:5.1f}% {forecast['price']:8.2f}\n")
                if len(tradeable) > 10:
                    f.write(f"    ... and {len(tradeable) - 10} more\n")
    
    print(f"üìã Created summary report: {summary_filename}")

def main():
    print("üîÑ Starting Direct Forecast Recovery...")
    print(f"Target dates: {', '.join(TARGET_DATES)}")
    
    total_recovered = 0
    
    for target_date in TARGET_DATES:
        try:
            forecasts = generate_forecasts_for_date(target_date)
            save_forecasts(forecasts, target_date)
            total_recovered += len(forecasts)
        except Exception as e:
            print(f"‚ùå Error recovering forecasts for {target_date}: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n‚úÖ Recovery complete! Total forecasts recovered: {total_recovered}")
    print("üìÅ Check data/recovered_forecasts/ for recovered forecast files")
    
    # Create overall summary
    output_dir = "data/recovered_forecasts"
    overall_summary = f"{output_dir}/RECOVERY_SUMMARY.txt"
    with open(overall_summary, 'w', encoding='utf-8') as f:
        f.write("Forecast Recovery Summary\n")
        f.write("=" * 40 + "\n\n")
        f.write(f"Recovered dates: {', '.join(TARGET_DATES)}\n")
        f.write(f"Total forecasts recovered: {total_recovered}\n")
        f.write(f"Average per date: {total_recovered/len(TARGET_DATES):.0f}\n\n")
        f.write("Files created:\n")
        for date in TARGET_DATES:
            f.write(f"  forecast_{date}.csv\n")
            f.write(f"  performance_log_{date}.csv\n")
            f.write(f"  summary_{date}.txt\n")
    
    print(f"üìä Overall summary saved to: {overall_summary}")

if __name__ == "__main__":
    main()
