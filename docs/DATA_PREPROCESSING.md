# Data Preprocessing Pipeline - Complete Guide

## üîÑ Overview

Data Preprocessing ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå

---

## üì¶ Pipeline Components

### **1. data_cleaner.py** - Data Cleaning

#### **Purpose:**
‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤

#### **Functions:**

##### **A. Remove Duplicates**
```python
def remove_duplicates(df):
    """
    ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (duplicate timestamps)
    """
    # ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    print(f"Before: {len(df)} rows")
    
    # ‡∏•‡∏ö duplicate index (‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
    df = df[~df.index.duplicated(keep='last')]
    
    print(f"After: {len(df)} rows")
    print(f"Removed: {removed} duplicates")
    
    return df
```

**Example:**
```
Before: 3005 rows
After:  3001 rows
Removed: 4 duplicates (same timestamp occurred twice)
```

##### **B. Remove NaN Values**
```python
def remove_nan(df):
    """
    ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢ (NaN)
    """
    # ‡∏Å‡πà‡∏≠‡∏ô
    print(f"NaN in close: {df['close'].isna().sum()}")
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN
    df = df.dropna()
    
    # ‡∏´‡∏•‡∏±‡∏á
    print(f"Clean data: {len(df)} rows")
    
    return df
```

**Example:**
```
NaN in close: 3 rows
NaN in volume: 1 row
Dropped: 4 rows
Clean data: 2997 rows
```

##### **C. Validate Price Range**
```python
def validate_prices(df):
    """
    ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
    """
    # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö?
    if (df['close'] < 0).any():
        print("‚ùå Negative prices found!")
        df = df[df['close'] >= 0]
    
    # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥? (>50% ‡πÉ‡∏ô 1 ‡∏ß‡∏±‡∏ô)
    pct_change = df['close'].pct_change() * 100
    outliers = df[abs(pct_change) > 50]
    
    if len(outliers) > 0:
        print(f"‚ö†Ô∏è {len(outliers)} extreme moves found")
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ circuit breaker = 30%
        # >50% ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
```

##### **D. Sort by Date**
```python
def sort_by_date(df):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡πÄ‡∏Å‡πà‡∏≤ ‚Üí ‡πÉ‡∏´‡∏°‡πà)
    """
    df = df.sort_index()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ñ‡∏π‡∏Å‡πÅ‡∏•‡πâ‡∏ß
    assert df.index.is_monotonic_increasing
    
    return df
```

---

### **2. batch_processor.py** - Batch Operations

#### **Purpose:**
‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

#### **Functions:**

##### **A. Batch Clean**
```python
def batch_clean(data_dir='data/stocks'):
    """
    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    """
    files = glob('data/stocks/*.parquet')
    
    for file in files:
        df = pd.read_parquet(file)
        
        # Clean
        df = remove_duplicates(df)
        df = remove_nan(df)
        df = sort_by_date(df)
        
        # Save cleaned
        df.to_parquet(file)
        
    print(f"‚úÖ Cleaned {len(files)} files")
```

##### **B. Batch Calculate**
```python
def batch_calculate_pct_change(data_dir):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì pct_change ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå
    """
    files = glob('data/stocks/*.parquet')
    
    for file in files:
        df = pd.read_parquet(file)
        
        # Calculate pct_change
        if 'pct_change' not in df.columns:
            df['pct_change'] = df['close'].pct_change() * 100
        
        # Save
        df.to_parquet(file)
```

---

### **3. data_cache.py** - Caching System

#### **Purpose:**
‡πÄ‡∏Å‡πá‡∏ö cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô

#### **Functions:**

##### **A. Load with Cache**
```python
def load_stock_cached(symbol, exchange):
    """
    Load ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° cache
    """
    cache_key = f"{symbol}_{exchange}"
    
    # Check cache first
    if cache_key in cache:
        return cache[cache_key]
    
    # Load from file
    df = pd.read_parquet(f'data/stocks/{cache_key}.parquet')
    
    # Store in cache
    cache[cache_key] = df
    
    return df
```

**Performance:**
```
First load:  0.05 seconds (from disk)
Cached:      0.001 seconds (50x faster!)
```

---

## üîÑ Complete Preprocessing Workflow

### **Step-by-Step:**

```python
# 1. Download Raw Data
df = tv.get_hist(symbol='PTT', exchange='SET', n_bars=3000)

# 2. Remove Duplicates
df = df[~df.index.duplicated(keep='last')]
# Before: 3005 rows ‚Üí After: 3001 rows

# 3. Remove NaN
df = df.dropna()
# Before: 3001 rows ‚Üí After: 2998 rows

# 4. Sort by Date
df = df.sort_index()
# Ensure: oldest ‚Üí newest

# 5. Validate Prices
assert (df['close'] > 0).all()
assert df['close'].max() < 1000  # Sanity check

# 6. Calculate pct_change
df['pct_change'] = df['close'].pct_change() * 100
# NaN for first row

# 7. Save to Parquet
df.to_parquet('data/stocks/PTT_SET.parquet')
```

---

## üìä Data Quality Checks

### **Automated Checks:**

```python
def validate_data_quality(df):
    """
    ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    checks = {
        'no_duplicates': not df.index.duplicated().any(),
        'no_nan': not df['close'].isna().any(),
        'sorted': df.index.is_monotonic_increasing,
        'positive_prices': (df['close'] > 0).all(),
        'volume_exists': (df['volume'] > 0).any(),
        'pct_change_calculated': 'pct_change' in df.columns
    }
    
    # Report
    for check, passed in checks.items():
        status = "‚úÖ" if passed else "‚ùå"
        print(f"{status} {check}")
    
    return all(checks.values())
```

**Example Output:**
```
‚úÖ no_duplicates
‚úÖ no_nan
‚úÖ sorted
‚úÖ positive_prices
‚úÖ volume_exists
‚úÖ pct_change_calculated

Result: PASS ‚úÖ
```

---

## üêõ Common Issues & Fixes

### **Issue 1: Duplicate Timestamps**
```python
# Problem
2026-01-14 09:00:00   32.75   100M
2026-01-14 09:00:00   32.80   50M   ‚Üê duplicate!

# Fix
df = df[~df.index.duplicated(keep='last')]
# Keep latest value: 32.80
```

### **Issue 2: Missing Data (NaN)**
```python
# Problem
2026-01-14   32.75   100M
2026-01-15   NaN     NaN     ‚Üê missing!
2026-01-16   33.00   80M

# Fix
df = df.dropna()
# Remove row with NaN
```

### **Issue 3: Wrong Order**
```python
# Problem
2026-01-15   32.75   ‚Üê latest first
2026-01-14   32.50
2026-01-13   31.75

# Fix
df = df.sort_index()
# Oldest ‚Üí Newest
2026-01-13   31.75
2026-01-14   32.50
2026-01-15   32.75
```

### **Issue 4: Extreme Values**
```python
# Problem
2026-01-14   32.75   +3%     ‚Üê normal
2026-01-15   65.50   +100%   ‚Üê error!

# Fix
pct = df['close'].pct_change() * 100
outliers = df[abs(pct) > 50]
print(f"Outliers: {outliers}")
# Manual review required
```

---

## üí° Best Practices

### **1. Always Clean Before Analysis**
```python
# Bad
df = pd.read_parquet('stock.parquet')
analyze(df)  # May have duplicates, NaN!

# Good
df = pd.read_parquet('stock.parquet')
df = clean_data(df)  # Clean first
analyze(df)  # Safe!
```

### **2. Validate After Download**
```python
# After download
df = download_stock('PTT')

# Immediate check
assert len(df) > 100  # Enough data?
assert not df['close'].isna().any()  # Complete?
assert df.index.is_monotonic_increasing  # Sorted?
```

### **3. Use Caching for Speed**
```python
# Slow (load every time)
for i in range(100):
    df = pd.read_parquet('PTT.parquet')  # 0.05s √ó 100 = 5s

# Fast (load once)
df = load_cached('PTT')  # 0.05s once
for i in range(100):
    use(df)  # 0.001s √ó 100 = 0.1s
```

---

## üéØ Real Example

### **PTT Data Processing:**

```python
# 1. Download
raw_df = download_from_tradingview('PTT')
print(f"Raw: {len(raw_df)} rows")
# Raw: 3005 rows

# 2. Clean
cleaned = remove_duplicates(raw_df)
print(f"After dedup: {len(cleaned)} rows")
# After dedup: 3001 rows

cleaned = cleaned.dropna()
print(f"After NaN: {len(cleaned)} rows")
# After NaN: 3001 rows (no NaN!)

# 3. Sort
cleaned = cleaned.sort_index()

# 4. Calculate
cleaned['pct_change'] = cleaned['close'].pct_change() * 100

# 5. Validate
assert validate_data_quality(cleaned)
print("‚úÖ Data quality: PASS")

# 6. Save
cleaned.to_parquet('data/stocks/PTT_SET.parquet')
print("üíæ Saved clean data")
```

**Result:**
```
Raw data:        3005 rows
Duplicates:      -4 rows
NaN values:       0 rows
Final:           3001 rows
Quality:         ‚úÖ PASS
```

---

## ‚úÖ Summary

### **Preprocessing Steps:**

1. **Download** - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
2. **Deduplicate** - ‡∏•‡∏ö‡∏ã‡πâ‡∏≥
3. **Remove NaN** - ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢
4. **Sort** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
5. **Validate** - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
6. **Calculate** - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì pct_change
7. **Save** - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å clean data

### **Tools:**
- `data_cleaner.py` - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
- `batch_processor.py` - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô batch
- `data_cache.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏î‡πâ‡∏ß‡∏¢ cache

### **Quality Checks:**
- ‚úÖ No duplicates
- ‚úÖ No NaN
- ‚úÖ Sorted by date
- ‚úÖ Positive prices
- ‚úÖ pct_change calculated

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ 100%!** üîÑ‚ú®
