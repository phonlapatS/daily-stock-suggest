#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
main.py - Fractal N+1 Prediction Runner (Categorized Edition)
=============================================================
Orchestrates the entire pipeline: 
Fetch -> Process -> Categorize -> Report -> Forget.
"""
import sys
import time
import os
import pandas as pd
from dotenv import load_dotenv
from tvDatafeed import TvDatafeed
import config
import processor
import requests
import re
from core.data_cache import (
    get_data_with_cache, 
    get_cache_stats, 
    has_cache, 
    is_cache_fresh, 
    load_cache, 
    is_connection_healthy,
    set_connection_healthy
)
from core.performance import log_forecast, verify_forecast

# Fix encoding for Windows console
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Load environment variables from .env file
load_dotenv()

# Rate Limiting Config (reduced since cache handles most requests)
REQUEST_DELAY = 0.3  # V4.8: Optimized for Delta fetch
BACKOFF_BASE = 2.0   # Exponential backoff multiplier

# Import thresholds from config (V6.0 - Configurable)
# ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô config.py ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ code
MIN_MATCHES_THRESHOLD = config.MIN_MATCHES_THRESHOLD
MIN_PROB_THRESHOLD = config.MIN_PROB_THRESHOLD
USE_TIER_CLASSIFICATION = config.USE_TIER_CLASSIFICATION

def _is_true_flag(val) -> bool:
    """
    Robust boolean coercion for values coming from:
    - engine results (bool)
    - CSV Smart Resume (strings / numbers / NaN)
    """
    if val is True:
        return True
    if val is False or val is None:
        return False
    # Pandas/NumPy NaN should be treated as False
    try:
        if pd.isna(val):
            return False
    except Exception:
        pass
    if isinstance(val, (int, float)):
        return val == 1
    if isinstance(val, str):
        v = val.strip().lower()
        return v in ("true", "1", "yes", "y", "t")
    return False

def fetch_and_analyze(tv, asset_info, history_bars, interval, fixed_threshold=None):
    """
    Fetch data with smart caching and analyze.
    All retry/timeout/fallback logic is handled by data_cache.py.
    """
    symbol = asset_info['symbol']
    exchange = asset_info['exchange']
    
    try:
        df = get_data_with_cache(
            tv=tv,
            symbol=symbol,
            exchange=exchange,
            interval=interval,
            full_bars=history_bars,
            delta_bars=50
        )
        
        if df is not None and not df.empty:
            results_list = processor.analyze_asset(df, symbol=symbol, exchange=exchange, fixed_threshold=fixed_threshold)
            display_name = asset_info.get('name', symbol)
            for res in results_list:
                res['symbol'] = display_name
            return results_list
        else:
            return None # V4.8: Return None to indicate fetch failure
            
    except Exception:
        return None



def show_all_forecasts(results):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å forecast ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏¢‡∏°‡∏≤ (‡∏Å‡∏£‡∏≠‡∏á matches ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
    """
    if not results:
        return
    
    # Use global threshold
    MIN_MATCHES = MIN_MATCHES_THRESHOLD
    
    print("\n" + "=" * 90)
    print(f"üìã ALL FORECASTS (Filtered: matches >= {MIN_MATCHES})")
    print("=" * 90)
    
    total_before_filter = 0
    total_after_filter = 0
    
    for group_key, settings in config.ASSET_GROUPS.items():
        title = settings['description'].upper()
        
        # Add Emoji based on group key
        if "THAI" in group_key: title = f"üáπüá≠ {title}"
        elif "US" in group_key: title = f"üá∫üá∏ {title}"
        elif "CHINA" in group_key: title = f"üá®üá≥ {title}"
        elif "INDICES" in group_key: title = f"üåç {title}"
        elif "METALS" in group_key: title = f"‚ö° {title}"
        
        group_results = [r for r in results if r['group'] == group_key]
        total_before_filter += len(group_results)
        
        # Filter: matches >= MIN_MATCHES (sample size ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ä‡∏ô‡∏∞‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ)
        filtered_results = [r for r in group_results if r.get('matches', 0) >= MIN_MATCHES]
        total_after_filter += len(filtered_results)
        
        if not filtered_results:
            continue
        
        print(f"\n{title}")
        print("-" * 110)
        header = f"{'Symbol':<12} {'Pattern':^10} {'Bull%':>8} {'Bear%':>8} {'Matches':>8} {'Total Bars':>12} {'Tradeable':>10} {'Price':>10}"
        print(header)
        print("-" * 110)
        
        # Sort by symbol and pattern
        filtered_results.sort(key=lambda x: (x.get('symbol', ''), x.get('pattern', '')))
        
        for r in filtered_results:
            symbol = r.get('symbol', 'Unknown')
            pattern = r.get('pattern', '???')
            prob = r.get('acc_score', 50.0)
            forecast = r.get('forecast_label', 'NEUTRAL')
            matches = r.get('total_events', 0)
            is_tradeable = _is_true_flag(r.get('is_tradeable', False))
            price = r.get('price', 0)
            
            tradeable_str = "‚úÖ YES" if is_tradeable else "‚ùå NO"
            
            print(f"{symbol:<12} {pattern:^10} {forecast:^10} {prob:>7.1f}% {int(matches):>8} {tradeable_str:>10} {price:>10.2f}")
        
        print("-" * 110)
        filtered_count = len(filtered_results)
        skipped_count = len(group_results) - filtered_count
        if skipped_count > 0:
            print(f"Total: {filtered_count} forecasts (skipped {skipped_count} with matches < {MIN_MATCHES})")
        else:
            print(f"Total: {filtered_count} forecasts")
    
    print("\n" + "=" * 90)
    if total_before_filter > total_after_filter:
        skipped_total = total_before_filter - total_after_filter
        print(f"üìä Summary: {total_after_filter} forecasts shown (skipped {skipped_total} with matches < {MIN_MATCHES})")
    else:
        print(f"üìä Summary: {total_after_filter} forecasts shown")
    print("=" * 90)

def _show_pending_verified_forecasts():
    """
    ‡πÅ‡∏™‡∏î‡∏á forecast ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á pending ‡∏´‡∏£‡∏∑‡∏≠ verified ‡∏à‡∏≤‡∏Å performance_log.csv
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ user ‡πÄ‡∏´‡πá‡∏ô forecast ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏ß‡πâ (‡πÄ‡∏ä‡πà‡∏ô GILD, TMUS, WHA)
    """
    import os
    from datetime import datetime, timedelta
    from core.performance import LOG_FILE
    
    if not os.path.exists(LOG_FILE):
        return
    
    try:
        perf_df = pd.read_csv(LOG_FILE)
        if perf_df.empty:
            return
        
        # Filter: target_date = ‡∏ß‡∏±‡∏ô‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ (forecast ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏£‡∏≠‡∏ú‡∏•)
        tomorrow = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')
        today = datetime.now().strftime('%Y-%m-%d')
        
        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ forecast ‡∏ó‡∏µ‡πà target_date = ‡∏ß‡∏±‡∏ô‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡∏¢‡∏±‡∏á‡∏£‡∏≠ verify)
        relevant = perf_df[
            (perf_df['target_date'] == tomorrow) | 
            ((perf_df['target_date'] == today) & (perf_df['actual'] == 'PENDING'))
        ].copy()
        
        if relevant.empty:
            return
        
        # Group by exchange/market
        pending_forecasts = []
        verified_forecasts = []
        
        for _, row in relevant.iterrows():
            forecast_data = {
                'symbol': row.get('symbol', ''),
                'exchange': row.get('exchange', ''),
                'pattern': row.get('pattern', ''),
                'forecast': row.get('forecast', ''),
                'prob': row.get('prob', 0),
                'matches': row.get('stats', 0),
                'actual': row.get('actual', 'PENDING'),
                'correct': row.get('correct', None),
                'target_date': row.get('target_date', '')
            }
            
            if row.get('actual') == 'PENDING':
                pending_forecasts.append(forecast_data)
            else:
                verified_forecasts.append(forecast_data)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤‡∏°‡∏µ forecast ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á pending ‡∏´‡∏£‡∏∑‡∏≠ verified
        if pending_forecasts or verified_forecasts:
            print("\n" + "=" * 90)
            print("üìã FORECASTS FROM FORWARD TESTING LOG")
            print("=" * 90)
            
            # Group by exchange
            from collections import defaultdict
            by_exchange = defaultdict(lambda: {'pending': [], 'verified': []})
            
            for f in pending_forecasts:
                ex = f['exchange']
                by_exchange[ex]['pending'].append(f)
            
            for f in verified_forecasts:
                ex = f['exchange']
                by_exchange[ex]['verified'].append(f)
            
            # Display by exchange
            for exchange, data in by_exchange.items():
                if not data['pending'] and not data['verified']:
                    continue
                
                # Exchange name
                if exchange == 'SET':
                    title = "üáπüá≠ THAI MARKET (SET)"
                elif exchange == 'NASDAQ':
                    title = "üá∫üá∏ US MARKET (NASDAQ)"
                elif exchange == 'HKEX':
                    title = "üá≠üá∞ HONG KONG (HKEX)"
                elif exchange == 'TWSE':
                    title = "üáπüáº TAIWAN (TWSE)"
                else:
                    title = f"üìä {exchange}"
                
                # Pending forecasts
                if data['pending']:
                    print(f"\n{title} - ‚è≥ PENDING")
                    print(f"{'Symbol':<12} {'Pattern':^10} {'Forecast':<15} {'Prob%':>8} {'Matches':>10} {'Target Date':<12}")
                    print("-" * 75)
                    for f in sorted(data['pending'], key=lambda x: (x['prob'], x['matches']), reverse=True):
                        forecast_str = f"üü¢ {f['forecast']}" if f['forecast'] == 'UP' else f"üî¥ {f['forecast']}"
                        print(f"{f['symbol']:<12} {f['pattern']:^10} {forecast_str:<15} {f['prob']:>7.1f}% {f['matches']:>10} {f['target_date']:<12}")
                    print("-" * 75)
                
                # Verified forecasts (‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà verify ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ)
                if data['verified']:
                    print(f"\n{title} - ‚úÖ VERIFIED")
                    print(f"{'Symbol':<12} {'Pattern':^10} {'Forecast':<15} {'Actual':<15} {'Result':<10} {'Prob%':>8}")
                    print("-" * 75)
                    for f in sorted(data['verified'], key=lambda x: x['symbol']):
                        forecast_str = f"üü¢ {f['forecast']}" if f['forecast'] == 'UP' else f"üî¥ {f['forecast']}"
                        actual_str = f"üü¢ {f['actual']}" if f['actual'] == 'UP' else f"üî¥ {f['actual']}"
                        result_str = "‚úÖ YES" if f['correct'] == 1 else "‚ùå NO"
                        print(f"{f['symbol']:<12} {f['pattern']:^10} {forecast_str:<15} {actual_str:<15} {result_str:<10} {f['prob']:>7.1f}%")
                    print("-" * 75)
            
            print("=" * 90)
    except Exception as e:
        # Silent fail - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á error ‡∏ñ‡πâ‡∏≤ performance_log.csv ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        pass

def generate_report(results):
    """
    V5.0: Single-Gate Report
    ========================
    ‡πÉ‡∏ä‡πâ is_tradeable ‡∏à‡∏≤‡∏Å Engine ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    Engine ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡πâ‡∏ß‡∏ô‡πÜ (WR, RRR, Count)
    ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏´‡∏∏‡πâ‡∏ô ‡∏ì ‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‚Äî purely data-driven
    """
    print("=" * 90)
    print(f"üìä PREDICT N+1 REPORT")
    print("=" * 90)
    
    for group_key, settings in config.ASSET_GROUPS.items():
        title = settings['description'].upper()
        
        # Add Emoji based on group key
        if "THAI" in group_key: title = f"üáπüá≠ {title}"
        elif "US" in group_key: title = f"üá∫üá∏ {title}"
        elif "CHINA" in group_key: title = f"üá®üá≥ {title}"
        elif "INDICES" in group_key: title = f"üåç {title}"
        elif "METALS" in group_key: title = f"‚ö° {title}"

        # -------------------------------------------------------------
        # 1. Forward Testing Display: ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏á RRR)
        # -------------------------------------------------------------
        # Forward Testing = ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡πÑ‡∏´‡∏°
        # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (RRR) - ‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á calculate_metrics.py
        # 
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•:
        #   - matches >= 30 (sample size ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ä‡∏ô‡∏∞‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ)
        #   - ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏ô‡∏∞ (prob ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤) ‡πÅ‡∏•‡∏∞‡∏ä‡∏ô‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (prob >= 55%, ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô >= 5%)
        #   - ‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏á RRR (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡πÑ‡∏´‡∏° ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û)
        # -------------------------------------------------------------
        
        filtered_data = []
        for r in results:
            if r['group'] != group_key: continue
            
            # V4.4: Use consolidated accuracy score from voting
            prob = r.get('acc_score', 0)
            
            # Simple threshold check: Must be clearly winning (>= 55%)
            if prob < 55:
                continue
                
            r['_sort_prob'] = prob
            filtered_data.append(r)
        
        if not filtered_data:
            print(f"\n{title}")
            print("   (No signals found)")
            continue
            
        print(f"\n{title}")
        
        # 2. Deduplication (V4.4: Best Fit per Symbol)
        seen_keys = {}
        for r in filtered_data:
            symbol = r.get('symbol', '')
            if symbol not in seen_keys or r['_sort_prob'] > seen_keys[symbol]['_sort_prob']:
                seen_keys[symbol] = r
        
        filtered_data = list(seen_keys.values())
        
        # 3. Sorting (Prob DESC)
        filtered_data.sort(key=lambda x: -x['_sort_prob'])
        
        # 4. Table Layout - V4.4 Simplified Voting Summary
        header = f"{'Symbol':<12} {'Forecast':^10} {'Prob%':>10}"
        
        print("-" * 40)
        print(header)
        print("-" * 40)

        for r in filtered_data:
            forecast = r.get('forecast_label', '')
            chance = "P" if forecast == 'UP' else "N"
            prob_val = r.get('acc_score', 50.0)
            
            print(f"{r['symbol']:<12} {chance:^10} {prob_val:>9.1f}%")
        print("-" * 40)

    # Export ALL results to CSV (both tradeable and not ‚Äî for analysis/debug)
    df = pd.DataFrame(results)
    df.to_csv('data/forecast_tomorrow.csv', index=False, encoding='utf-8-sig')
    tradeable_count = sum(1 for r in results if _is_true_flag(r.get('is_tradeable', False)))
    print(f"\nüíæ Saved {len(results)} patterns ({tradeable_count} tradeable) to data/forecast_tomorrow.csv")
    
    # V5.3: ‡πÅ‡∏™‡∏î‡∏á forecast ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á pending/verified ‡∏à‡∏≤‡∏Å performance_log.csv
    _show_pending_verified_forecasts()
    
    print("\n‚úÖ Report Generated.")

def get_auth_token(session_id):
    """
    Exchange session_id cookie for a valid auth_token by scraping the TradingView homepage.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Cookie': f'sessionid={session_id}'
    }
    try:
        response = requests.get('https://www.tradingview.com/', headers=headers, timeout=10)
        if response.status_code == 200:
            # Look for auth_token in the page source
            match = re.search(r'"auth_token":"(.*?)"', response.text)
            if match:
                return match.group(1)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to exchange Session ID for Token: {e}")
    return None

def main():
    import time
    import os
    start_time = time.time()
    
    
    print("üöÄ Starting Fractal N+1 Prediction System...")
    
    # Connect TV - Prioritize Session ID for stability
    tv = TvDatafeed() # Default to guest first
    
    try:
        session_id = os.getenv("TV_SESSIONID")
        username = os.getenv("TV_USERNAME")
        password = os.getenv("TV_PASSWORD")
        
        token = None
        if session_id:
            print("üç™ Found Session ID. Fetching Auth Token...")
            token = get_auth_token(session_id)
            if token:
                tv.token = token
                print("‚úÖ Authenticated via Session ID!")
            else:
                print("‚ö†Ô∏è Invalid Session ID or Token not found. Falling back...")
        
        # Fallback to User/Pass if no session or failed
        if not token and username and password:
             # Try legacy login but catch errors
             try: 
                print(f"üîê Logging in as {username}...")
                tv = TvDatafeed(username, password)
             except Exception as e:
                print(f"‚ö†Ô∏è User/Pass login failed: {e}")
             
    except Exception as e:
        print(f"‚ùå Connection Failed: {e}")
        return

    # =========================================================
    # STARTUP: Legacy cleanup + Health check + Cache stats
    # =========================================================
    from core.data_cache import cleanup_legacy_pkl, check_connection_health, get_cache_stats
    
    # Clean up old .pkl files (runs once, harmless if none exist)
    cleanup_legacy_pkl()
    
    # Quick connection test
    if not check_connection_health(tv):
        print("‚ö†Ô∏è Connection unstable ‚Äî will rely on cached data where possible")
    else:
        print("‚úÖ TradingView connection healthy")
    
    # Show cache status
    cache_info = get_cache_stats()
    print(f"üì¶ Cache: {cache_info['total_files']} files ({cache_info['fresh']} fresh, {cache_info['stale']} stale) | {cache_info['total_size_mb']} MB")

    all_results = []
    
    # =========================================================
    # Session-level tracking: ‡πÄ‡∏Å‡πá‡∏ö symbols ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô session ‡∏ô‡∏µ‡πâ (V5.2)
    # =========================================================
    session_fetched = set()  # Track symbols fetched in this session
    
    # =========================================================
    # Skip symbols already scanned for today's target date (V5.2)
    # =========================================================
    import datetime
    from datetime import timedelta
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    yesterday_str = (datetime.datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    results_file = "data/forecast_tomorrow.csv"
    perf_log_file = "logs/performance_log.csv"
    already_scanned = set()
    
    # V5.2: Logic - ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏£‡∏±‡∏ô ‚Üí ‡∏ó‡∏≤‡∏¢‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‚Üí ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ô‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö ‚Üí skip
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å performance_log.csv (‡∏°‡∏µ target_date) ‡∏´‡∏£‡∏∑‡∏≠ forecast_tomorrow.csv (file timestamp)
    
    # Priority 1: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å performance_log.csv (‡∏°‡∏µ scan_date column)
    if os.path.exists(perf_log_file):
        try:
            perf_df = pd.read_csv(perf_log_file)
            if not perf_df.empty:
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ scan_date = ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (scan ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß)
                if 'scan_date' in perf_df.columns:
                    scan_dates = perf_df['scan_date'].unique()
                    if today_str in scan_dates:
                        # CSV scan ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß ‚Üí skip symbols ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                        if 'symbol' in perf_df.columns:
                            csv_symbols = set(perf_df[perf_df['scan_date'] == today_str]['symbol'].unique())
                            already_scanned.update(csv_symbols)
                            print(f"‚ö° Smart Resume: Found {len(already_scanned)} symbols already scanned today (from performance_log). Skipping them!")
                # Fallback: ‡πÄ‡∏ä‡πá‡∏Ñ target_date = ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡∏ó‡∏≤‡∏¢‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ)
                elif 'target_date' in perf_df.columns:
                    target_dates = perf_df['target_date'].unique()
                    if today_str in target_dates:
                        if 'symbol' in perf_df.columns:
                            csv_symbols = set(perf_df[perf_df['target_date'] == today_str]['symbol'].unique())
                            already_scanned.update(csv_symbols)
                            print(f"‚ö° Smart Resume: Found {len(already_scanned)} symbols already scanned for target date {today_str}. Skipping them!")
        except Exception:
            pass
    
    # Priority 2: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å forecast_tomorrow.csv (file timestamp)
    # V5.2: ‡πÄ‡∏û‡∏¥‡πà‡∏° logic ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡∏•‡∏≤‡∏î‡∏õ‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
    forecast_df = None
    perf_log_df = None
    csv_results_for_display = []  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏à‡∏≤‡∏Å CSV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á (‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà)
    
    if os.path.exists(results_file):
        try:
            file_mtime = datetime.datetime.fromtimestamp(os.path.getmtime(results_file))
            file_date = file_mtime.strftime("%Y-%m-%d")
            forecast_df = pd.read_csv(results_file)
            
            if not forecast_df.empty and 'symbol' in forecast_df.columns:
                # Pre-load cached results with NaN protection (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•)
                for _, row in forecast_df.iterrows():
                    d = row.to_dict()
                    if pd.isna(d.get('total_bars')): d['total_bars'] = 0
                    d['is_tradeable'] = _is_true_flag(d.get('is_tradeable', False))
                    csv_results_for_display.append(d)
                
                # ‡∏ñ‡πâ‡∏≤ CSV ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‚Üí ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ symbols ‡∏Ñ‡∏£‡∏ö‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‚Üí skip
                if file_date == today_str:
                    csv_symbols = set(forecast_df['symbol'].unique())
                    if len(csv_symbols) >= 20:  # Threshold: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ symbols ‡∏Ñ‡∏£‡∏ö‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‚Üí skip
                        already_scanned.update(csv_symbols)
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ all_results ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                        all_results.extend(csv_results_for_display)
                        print(f"‚ö° Smart Resume: Found {len(csv_symbols)} symbols in forecast CSV (file date: {file_date}). Added to skip list!")
        except Exception:
            pass  # If file is corrupted, scan everything fresh
    
    # Priority 3: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å cache files ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (V5.2 - New!)
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ cache fresh ‚Üí skip (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á fetch ‡πÉ‡∏´‡∏°‡πà)
    from core.data_cache import has_cache, is_cache_fresh
    cache_skipped = 0
    for group_name, settings in config.ASSET_GROUPS.items():
        for asset in settings['assets']:
            symbol = asset['symbol']
            exchange = asset.get('exchange', 'SET')
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ cache ‡πÅ‡∏•‡∏∞ fresh
            if has_cache(symbol, exchange) and is_cache_fresh(symbol, exchange):
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô already_scanned
                if symbol not in already_scanned:
                    already_scanned.add(symbol)
                    cache_skipped += 1
    
    if cache_skipped > 0:
        print(f"‚ö° Smart Resume: Found {cache_skipped} symbols with fresh cache. Added to skip list!")
    
    # Load perf_log_df for market time check
    if os.path.exists(perf_log_file):
        try:
            perf_log_df = pd.read_csv(perf_log_file)
        except Exception:
            pass
    
    if already_scanned:
        print(f"‚ö° Smart Resume: Found {len(already_scanned)} symbols already scanned today (from cache + CSV). Skipping them!")
    
    # Fetch Summary Tracking
    fetch_summary = {
        'total': 0,
        'success': 0,
        'failed': 0,
        'failed_symbols': [],
        'skipped': 0
    }
    
    # Global Price Map for Performance Update (N+1)
    price_map = {} # symbol -> latest_price
    
    consecutive_failures = 0
    
    # Iterate through Asset Groups
    for group_name, settings in config.ASSET_GROUPS.items():
        print(f"\nüìÇ Processing {settings['description']}...")
        
        assets = settings['assets']
        interval = settings['interval']
        history = settings['history_bars']
        
        for i, asset in enumerate(assets):
            display_name = asset.get('name', asset['symbol'])
            
            # V5.0: Smart Cooldown - ‡∏ñ‡πâ‡∏≤ connection bad ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á cooldown (‡πÉ‡∏ä‡πâ cache ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
            if consecutive_failures >= 10 and is_connection_healthy():
                print(f"\n‚ö†Ô∏è Too many failures ({consecutive_failures}). Pausing for 10s and reconnecting...")
                time.sleep(10) # Reduced from 20s
                # Re-initialize TvDatafeed
                tv = TvDatafeed()
                consecutive_failures = 0
            elif consecutive_failures >= 10:
                # Connection bad ‚Üí switch to cache-only mode (no cooldown needed)
                print(f"\n‚ö†Ô∏è Connection unstable. Switching to cache-only mode...")
                set_connection_healthy(False)
                consecutive_failures = 0
            
            # SMART SKIP: Multi-level check (V5.2)
            # 1. Session-level: already fetched in this session
            # 2. Day-level: already scanned today
            # 3. Market-time check: ‡∏°‡∏µ forecast ‡πÅ‡∏•‡πâ‡∏ß + ‡∏ï‡∏•‡∏≤‡∏î‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î ‚Üí skip
            symbol_upper = asset['symbol'].upper()
            display_upper = display_name.upper() if display_name else symbol_upper
            exchange = asset.get('exchange', '')
            
            # Level 1: Session-level skip
            if (symbol_upper in session_fetched or asset['symbol'] in session_fetched):
                sys.stdout.write(f"\r   [{i+1}/{len(assets)}] ‚ö° {asset['symbol']} (already fetched in session)")
                sys.stdout.flush()
                fetch_summary['skipped'] += 1
                fetch_summary['total'] += 1
                continue
            
            # Level 2: Day-level skip (already scanned today)
            if (symbol_upper in already_scanned or 
                display_upper in already_scanned or
                asset['symbol'] in already_scanned or
                display_name in already_scanned):
                sys.stdout.write(f"\r   [{i+1}/{len(assets)}] ‚ö° {asset['symbol']} (already scanned today)")
                sys.stdout.flush()
                fetch_summary['skipped'] += 1
                fetch_summary['total'] += 1
                continue
            
            # Level 3: Market-time check (V5.2 - New!)
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ forecast ‡πÅ‡∏•‡πâ‡∏ß + ‡∏ï‡∏•‡∏≤‡∏î‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î ‚Üí skip (‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏•‡∏≤‡∏î‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô)
            from core.market_time import should_skip_symbol
            should_skip, skip_reason = should_skip_symbol(
                asset['symbol'], 
                exchange, 
                forecast_df=forecast_df,
                perf_log_df=perf_log_df
            )
            if should_skip:
                sys.stdout.write(f"\r   [{i+1}/{len(assets)}] ‚è∏Ô∏è {asset['symbol']} ({skip_reason})")
                sys.stdout.flush()
                fetch_summary['skipped'] += 1
                fetch_summary['total'] += 1
                continue
            
            sys.stdout.write(f"\r   [{i+1}/{len(assets)}] Scanning {asset['symbol']}...")
            sys.stdout.flush()
            
            fetch_summary['total'] += 1
            
            # Check for fixed threshold override in config
            fixed_thresh = settings.get('fixed_threshold', None)
            
            # V5.0: Smart Fetch - ‡πÄ‡∏ä‡πá‡∏Ñ cache ‡∏Å‡πà‡∏≠‡∏ô, retry ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            # Fast path: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ cache fresh ‡πÅ‡∏•‡∏∞ connection bad ‚Üí ‡πÉ‡∏ä‡πâ cache ‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á fetch)
            symbol = asset['symbol']
            exchange = asset['exchange']
            has_fresh_cache = has_cache(symbol, exchange) and is_cache_fresh(symbol, exchange)
            connection_bad = not is_connection_healthy()
            
            if has_fresh_cache and connection_bad:
                # ‡πÉ‡∏ä‡πâ cache ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á fetch
                cached_df = load_cache(symbol, exchange)
                if cached_df is not None and not cached_df.empty:
                    results_list = processor.analyze_asset(cached_df, symbol=symbol, exchange=exchange, fixed_threshold=fixed_thresh)
                    display_name = asset.get('name', symbol)
                    for res in results_list:
                        res['symbol'] = display_name
                    pattern_results = results_list
                    # Mark as fetched (‡πÉ‡∏ä‡πâ cache = ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
                    session_fetched.add(symbol_upper)
                    session_fetched.add(asset['symbol'])
                else:
                    pattern_results = None
            else:
                # Normal fetch (data_cache.py ‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ retry ‡πÄ‡∏≠‡∏á)
                pattern_results = fetch_and_analyze(tv, asset, history, interval, fixed_thresh)
            
            # Update results
            if pattern_results is not None:
                fetch_summary['success'] += 1
                consecutive_failures = 0 # Reset on success
                # Mark as fetched in this session (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ mark ‡∏à‡∏≤‡∏Å cache path)
                session_fetched.add(symbol_upper)
                session_fetched.add(asset['symbol'])
                for res in pattern_results:
                    res['group'] = group_name
                    res['exchange'] = asset['exchange'] # Add actual exchange
                    all_results.append(res)
                    # Update Price Map for Global Homework Check
                    price_map[res['symbol']] = res['price']
            else:
                fetch_summary['failed'] += 1
                consecutive_failures += 1 # Increment failure count
                # ‡πÑ‡∏°‡πà mark ‡πÄ‡∏õ‡πá‡∏ô fetched (‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤ retry)
                fetch_summary['failed_symbols'].append(asset['symbol'])
            
            # Rate limiting: ‡∏ñ‡πâ‡∏≤ connection bad ‚Üí delay ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á (‡πÉ‡∏ä‡πâ cache ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
            delay = 0.1 if connection_bad else REQUEST_DELAY
            time.sleep(delay)
    
    # Print Fetch Summary
    print("\n")
    print("=" * 50)
    print("üìä FETCH SUMMARY")
    print("=" * 50)
    success_rate = (fetch_summary['success'] / fetch_summary['total'] * 100) if fetch_summary['total'] > 0 else 0
    print(f"‚úÖ Success: {fetch_summary['success']}/{fetch_summary['total']} ({success_rate:.1f}%)")
    print(f"‚ö° Skipped: {fetch_summary['skipped']} (already scanned today)")
    print(f"‚ùå Failed:  {fetch_summary['failed']}")
    if fetch_summary['failed_symbols']:
        # Show first 10 failed symbols
        shown = fetch_summary['failed_symbols'][:10]
        print(f"   Failed symbols: {', '.join(shown)}", end='')
        if len(fetch_summary['failed_symbols']) > 10:
            print(f" ... and {len(fetch_summary['failed_symbols']) - 10} more")
        else:
            print()
    print("=" * 50)
    
    # -------------------------------------------------------------
    # Forward Testing: ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô (Always run, even if no new results)
    # -------------------------------------------------------------
    # V5.0: Forward Testing - ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô + Log ‡πÉ‡∏´‡∏°‡πà
    # -------------------------------------------------------------
    # Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô (verify forecasts ‡∏ó‡∏µ‡πà target_date <= ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ)
    print("\n" + "="*80)
    print("üìä Forward Testing: ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á vs ‡∏ó‡∏≤‡∏¢)")
    print("="*80)
    try:
        verify_result = verify_forecast(tv=tv)
        if verify_result:
            verified = verify_result.get('verified', 0)
            correct = verify_result.get('correct', 0)
            incorrect = verify_result.get('incorrect', 0)
            if verified > 0:
                accuracy = (correct / verified * 100) if verified > 0 else 0
                print(f"‚úÖ Verified: {verified} forecasts | ‚úÖ Correct: {correct} | ‚ùå Incorrect: {incorrect} | üìà Accuracy: {accuracy:.1f}%")
            else:
                print("‚ÑπÔ∏è No pending forecasts to verify (all are already verified or target_date is in future)")
    except Exception as e:
        print(f"‚ö†Ô∏è Verify failed: {e}")
    
    # Final Report
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà ‚Üí ‡πÉ‡∏ä‡πâ all_results
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏à‡∏≤‡∏Å CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ user ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏¢‡∏≠‡∏∞‡πÑ‡∏£)
    display_results = all_results if all_results else csv_results_for_display
    
    if display_results:
        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PREDICT N+1 REPORT (‡∏°‡∏µ Forecast ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô UP/DOWN)
        # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á ALL FORECASTS ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏≠‡∏Å‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
        generate_report(display_results)

        # Step 3: Log forecasts based on configurable thresholds (V6.0)
        # Note: Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà (all_results) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏à‡∏≤‡∏Å CSV
        # V6.0: ‡πÉ‡∏ä‡πâ Prob > MIN_PROB_THRESHOLD + Matches >= MIN_MATCHES_THRESHOLD
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ is_tradeable (Prob‚â•60%) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤
        if all_results:
            try:
                # Filter by configurable thresholds
                eligible = []
                for r in all_results:
                    # Get the winning side (higher prob)
                    bull_prob = r.get('bull_prob', 50)
                    bear_prob = r.get('bear_prob', 50)
                    max_prob = max(bull_prob, bear_prob)
                    matches = r.get('matches', 0)
                    
                    # Check thresholds
                    if max_prob > MIN_PROB_THRESHOLD and matches >= MIN_MATCHES_THRESHOLD:
                        # Add tier classification if enabled
                        if USE_TIER_CLASSIFICATION:
                            r['tier'] = 'A' if max_prob >= 60.0 else 'B'
                        # Store max_prob for deduplication
                        r['_max_prob'] = max_prob
                        eligible.append(r)
                
                # V6.1: Deduplicate - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (symbol, pattern, forecast) ‡∏ã‡πâ‡∏≥ ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ max_prob ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                if eligible:
                    from collections import defaultdict
                    dedup_dict = defaultdict(list)
                    
                    # Group by (symbol, pattern, forecast)
                    for r in eligible:
                        symbol = r.get('symbol', '')
                        pattern = r.get('pattern', '')
                        forecast = r.get('forecast_label', 'UP')
                        key = (symbol, pattern, forecast)
                        dedup_dict[key].append(r)
                    
                    # Select best (highest acc_score) for each group
                    deduplicated = []
                    for key, records in dedup_dict.items():
                        if len(records) > 1:
                            # Sort by acc_score descending, then by total_events descending
                            records.sort(key=lambda x: (x.get('acc_score', 0), x.get('total_events', 0)), reverse=True)
                            # Keep only the best one
                            deduplicated.append(records[0])
                        else:
                            deduplicated.append(records[0])
                    
                    if deduplicated:
                        log_forecast(deduplicated)
                        tier_a = sum(1 for r in deduplicated if r.get('tier') == 'A') if USE_TIER_CLASSIFICATION else 0
                        tier_b = sum(1 for r in deduplicated if r.get('tier') == 'B') if USE_TIER_CLASSIFICATION else 0
                        if USE_TIER_CLASSIFICATION:
                            print(f"üìù Logged {len(deduplicated)} new forecasts (Tier A: {tier_a}, Tier B: {tier_b}) for verification tomorrow")
                        else:
                            print(f"üìù Logged {len(deduplicated)} new forecasts (Prob>{MIN_PROB_THRESHOLD}%, Matches>={MIN_MATCHES_THRESHOLD}) for verification tomorrow")
            except Exception as e:
                print(f"‚ö†Ô∏è Forward log failed: {e}")
        
        # -------------------------------------------------------------
        # 4. Global Homework Check (Check ALL files in logs)
        # -------------------------------------------------------------
        # Note: ‡πÉ‡∏ä‡πâ price_map ‡∏à‡∏≤‡∏Å all_results ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà)
        if all_results:
            from scripts.stock_logger import StockLogger
            stock_logger = StockLogger()
            
            # Scan ALL market subdirectories for OPEN trades to close with today's price
            if os.path.exists(stock_logger.log_dir):
                for m_dir in [d for d in os.listdir(stock_logger.log_dir) if os.path.isdir(os.path.join(stock_logger.log_dir, d))]:
                    full_m_dir = os.path.join(stock_logger.log_dir, m_dir)
                    for f in os.listdir(full_m_dir):
                        if f.endswith(".csv"):
                            # Extract symbol from filename (handling sanitized names)
                            symbol = f.replace(".csv", "").split("_")[0]
                            if symbol in price_map:
                                stock_logger.close_trade(symbol, price_map[symbol], market=m_dir, silent=True)

            # -------------------------------------------------------------
            # 5. Log New Signals (Background Operation)
            # -------------------------------------------------------------
            # V5.0: Single Gate ‚Äî log only is_tradeable signals
            for r in all_results:
                if not _is_true_flag(r.get('is_tradeable', False)):
                    continue
                
                # V4.4: Use forecast_label for logging instead of direct avg_return
                forecast_label = r.get('forecast_label', 'UP')
                prob = r.get('acc_score', 0)
                
                # Market mapping for logging
                group = r['group']
                if "THAI" in group: m_name = "SET"
                elif "US" in group: m_name = "NASDAQ"
                elif "CHINA" in group: m_name = "HKEX"
                elif "TAIWAN" in group: m_name = "TWSE"
                elif "METALS" in group: m_name = "GOLD"
                else: m_name = "Other"

                stock_logger.log_trade(
                    symbol=r['symbol'], 
                    signal=forecast_label, 
                    entry_price=r['price'],
                    market=m_name,
                    silent=True
                )
        
        # -------------------------------------------------------------
        # 6. Market Health Summary (The "Heartbeat")
        # -------------------------------------------------------------
        # V5.0: Heartbeat counts ALL patterns found (success) 
        # but only is_tradeable as actionable UP/DOWN signals
        import datetime
        market_stats = {}
        
        for r in display_results:
            group = r['group']
            if "THAI" in group: m_key = "SET"
            elif "US" in group: m_key = "NASDAQ"
            elif "CHINA" in group: m_key = "HKEX"
            elif "TAIWAN" in group: m_key = "TWSE"
            elif "METALS" in group: m_key = "GOLD"
            else: m_key = "Other"
            
            if m_key not in market_stats:
                market_stats[m_key] = {'scanned': 0, 'tradeable': 0, 'up': 0, 'down': 0, 'best_pattern': '', 'best_prob': 0}
            
            market_stats[m_key]['scanned'] += 1
            
            # Only count is_tradeable as actionable signals
            if not _is_true_flag(r.get('is_tradeable', False)):
                continue
                
            market_stats[m_key]['tradeable'] += 1
            
            forecast_label = r.get('forecast_label', 'UP')
            prob = r.get('acc_score', 0)
            
            # Track best pattern per market
            if prob > market_stats[m_key]['best_prob']:
                market_stats[m_key]['best_prob'] = prob
                pat_display = r.get('pattern', r.get('pattern_display', '???'))
                market_stats[m_key]['best_pattern'] = f"{r['symbol']} ({pat_display}) {prob:.0f}%"

            if forecast_label == 'UP': 
                market_stats[m_key]['up'] += 1
            else: 
                market_stats[m_key]['down'] += 1

        # Print Heartbeat Table
        if market_stats:
            now_dt = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
            print("\n" + "=" * 85)
            print(f"üè• SYSTEM HEARTBEAT | Last Update: {now_dt}")
            print("=" * 85)
            print(f"{'Market':<10} {'Patterns':>10} {'Tradeable':>10} {'UP':>6} {'DOWN':>6}   {'Top Signal'}")
            print("-" * 85)
            
            for m_key, s in market_stats.items():
                print(f"{m_key:<10} {s['scanned']:>10} {s['tradeable']:>10} {s['up']:>6} {s['down']:>6}   {s['best_pattern']}")
            print("-" * 85)
            print("‚úÖ Reports & Logs updated.")
            
            # Save Heartbeat to file
            with open("data/system_heartbeat.txt", "w", encoding="utf-8") as f:
                f.write(f"SYSTEM HEARTBEAT | Updated: {now_dt}\n")
                f.write("=" * 85 + "\n")
                f.write(f"{'Market':<10} {'Patterns':>10} {'Tradeable':>10} {'UP':>6} {'DOWN':>6}   {'Top Signal'}\n")
                for m_key, s in market_stats.items():
                    f.write(f"{m_key:<10} {s['scanned']:>10} {s['tradeable']:>10} {s['up']:>6} {s['down']:>6}   {s['best_pattern']}\n")
                f.write("-" * 85 + "\n")

            # -------------------------------------------------------------
            # 7. Final Status
            # -------------------------------------------------------------
            if all_results:
                from scripts.stock_logger import StockLogger
                stock_logger = StockLogger()
                print(f"‚úÖ All systems updated. Logs synced to {stock_logger.log_dir}")

    else:
        print("\n‚ùå No matching patterns found in any asset (and no CSV data available).")
    
    # Print execution time
    end_time = time.time()
    duration = end_time - start_time
    minutes = int(duration // 60)
    seconds = int(duration % 60)
    print(f"\n‚è±Ô∏è Total execution time: {minutes}m {seconds}s")

if __name__ == "__main__":
    main()
